{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b90043b3",
   "metadata": {},
   "source": [
    "## Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed98734f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are CodeFixerGPT, an expert software engineer that helps users debug and optimize their code. \n",
      "You explain your reasoning clearly, ask clarifying questions when needed, and always \n",
      "provide complete, working examples.\n"
     ]
    }
   ],
   "source": [
    "system_prompt=\"You are CodeFixerGPT, an expert software engineer that helps users debug and optimize their code. \\nYou explain your reasoning clearly, ask clarifying questions when needed, and always \\nprovide complete, working examples.\"\n",
    "print(system_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a69d5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's my Python code for sorting a list of dictionaries by the \"age\" key. \n",
      "It doesn't seem to work correctly — can you fix it?\n",
      "\n",
      "data = [{\"name\": \"Alice\", \"age\": 30}, {\"name\": \"Bob\", \"age\": 25}]\n",
      "data.sort(\"age\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_prompt=\"\"\"Here's my Python code for sorting a list of dictionaries by the \"age\" key. \\nIt doesn't seem to work correctly — can you fix it?\n",
    "\n",
    "data = [{\"name\": \"Alice\", \"age\": 30}, {\"name\": \"Bob\", \"age\": 25}]\n",
    "data.sort(\"age\")\n",
    "\"\"\"\n",
    "print(user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdba6fd",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c118b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"['Hugging Face is an open-source company that develops and maintains the Hugging Face platform, which is a collection of tools and libraries for building and deploying natural language processing (NLP) models. Hugging Face was founded in 2018 by Thomas Wolf']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208fef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if assistant_prompt:\n",
    "    prompt=f'''<|start_header_id|>system<|end_header_id|>\\n\\n\n",
    "        { system_prompt }<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\n",
    "        {user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{assistant_prompt}<|eot_id|>'''\n",
    "else:\n",
    "    prompt=f'''<|start_header_id|>system<|end_header_id|>\\n\\n\n",
    "        { system_prompt }<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\n",
    "        { user_prompt }<|eot_id|><|start_header_id|>assistant<|end_header_id|>'''\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1862ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "chat = [\n",
    "  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "  {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
    "]\n",
    "\n",
    "tokenizer.apply_chat_template(chat, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987a287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0799a8b",
   "metadata": {},
   "source": [
    "## Loading Open Source LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85a2d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8698cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For gated models, you need to login to access the model\n",
    "from huggingface_hub import login\n",
    "token=\"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\"\n",
    "login(token = token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230ac7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"google/gemma-2b-it\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "#Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model,truncation_side=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7c66c8",
   "metadata": {},
   "source": [
    "### Loading optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a14e607",
   "metadata": {},
   "source": [
    "datatype -- lower the precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b94fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c08370",
   "metadata": {},
   "source": [
    "change device to use gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00d45f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    device_map=\"cuda:0\",,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c146f17e",
   "metadata": {},
   "source": [
    "change attention type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf8f6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2637da58",
   "metadata": {},
   "source": [
    "#### Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218760e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"cuda:0\", ## works only with GPU\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eff10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n",
    "device_map = {\n",
    "    \"transformer.word_embeddings\": 0,\n",
    "    \"transformer.word_embeddings_layernorm\": 0,\n",
    "    \"lm_head\": \"cpu\",\n",
    "    \"transformer.h\": 0,\n",
    "    \"transformer.ln_f\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e0b7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.dequantize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9062f67",
   "metadata": {},
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff827f48",
   "metadata": {},
   "source": [
    "### Greedy search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35bbf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "# explicitly set to default length because Llama2 generation length is 4096\n",
    "outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe11a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Hugging Face is an open-source company that provides a suite of tools and services for building, deploying, and maintaining natural language processing'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146711e3",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51facc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(**inputs, max_new_tokens=50, do_sample=True, num_beams=1)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab5b19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Hugging Face is an open-source company 🤗\\nWe are open-source and believe that open-source is the best way to build technology. Our mission is to make AI accessible to everyone, and we believe that open-source is the best way to achieve that.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5403a0",
   "metadata": {},
   "source": [
    "### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931284b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(**inputs, max_new_tokens=50, num_beams=2)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb4a058",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cf9b05",
   "metadata": {},
   "source": [
    "### Dataset Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d1bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are helpful\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are helpful\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are helpful\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b0b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952870ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is an instruction ...\n",
    "\n",
    "### Instruction\n",
    "# {prompt}\n",
    "\n",
    "### Response:\n",
    "# {completion}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1661b2aa",
   "metadata": {},
   "source": [
    "### Full tuning (Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43321915",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=5e-5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    \n",
    "    report_to=[\"wandb\"],\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"eval\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d508c9",
   "metadata": {},
   "source": [
    "### Full tuning (SFT Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5885d9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=epochs,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy='epoch',  \n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=lr,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    report_to=[\"wandb\"],\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    max_seq_length=context_length,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing= False,   \n",
    "    # data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd2aace",
   "metadata": {},
   "source": [
    "## PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbea712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00634278",
   "metadata": {},
   "source": [
    "## QLORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046f3de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model= AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c650bf",
   "metadata": {},
   "source": [
    "## ✅ Advantages of QLoRA\n",
    "\n",
    "- 🔋 **Low memory usage**: Fine-tunes large models (e.g. LLaMA 13B/65B) on consumer GPUs (e.g. 24GB VRAM).\n",
    "- ⚡ **Efficient training**: Only small adapter weights are updated, making training faster and cheaper.\n",
    "- 🎯 **Competitive accuracy**: Achieves similar performance to full fine-tuning on many benchmarks.\n",
    "- 🧩 **Modular and reusable**: LoRA adapters are small, easy to store/share, and support multi-task fine-tuning.\n",
    "- 🔁 **Supports multiple adapters**: Load and switch between domain-specific adapters without retraining the base model.\n",
    "- 🧠 **Keeps base model intact**: Fine-tuning does not modify pretrained weights — good for safety and reproducibility.\n",
    "\n",
    "## ❌ Disadvantages of QLoRA\n",
    "\n",
    "- 🎯 **Limited adaptability**: Only fine-tunes a subset of weights — can underperform on complex or domain-shifted tasks.\n",
    "- 🧮 **Quantization noise**: 4-bit approximation may reduce precision, especially in less common tasks.\n",
    "- 🧰 **Tooling complexity**: Requires managing quantization, adapter configs, and training-specific frameworks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5bd5e1",
   "metadata": {},
   "source": [
    "## More Fast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66baca03",
   "metadata": {},
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a9b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, FastModel\n",
    "import torch\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset\n",
    "max_seq_length = 2048 # Supports RoPE Scaling internally, so choose any!\n",
    "# Get LAION dataset\n",
    "url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n",
    "dataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train\")\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # 4bit for 405b!\n",
    "    \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "\n",
    "    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n",
    "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "\n",
    "    \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\" # NEW! Llama 3.3 70B!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4B-it\",\n",
    "    max_seq_length = 2048, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    ")\n",
    "\n",
    "# Do model patching and add fast LoRA weights\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    max_seq_length = max_seq_length,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    args = SFTConfig(\n",
    "        max_seq_length = max_seq_length,\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 10,\n",
    "        max_steps = 60,\n",
    "        logging_steps = 1,\n",
    "        output_dir = \"outputs\",\n",
    "        optim = \"adamw_8bit\",\n",
    "        seed = 3407,\n",
    "    ),\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18658c4d",
   "metadata": {},
   "source": [
    "## 🚀 How Unsloth Achieves Faster Fine-Tuning\n",
    "\n",
    "### ✅ 1. Static Linear Rewriting\n",
    "- Replaces standard `torch.nn.Linear` with optimized LoRA + quantization fused layer.\n",
    "- Eliminates dynamic injection overhead.\n",
    "\n",
    "### ✅ 2. Flash Attention v2\n",
    "- Speeds up attention via memory-efficient CUDA kernels.\n",
    "- Great for long context lengths.\n",
    "\n",
    "### ✅ 3. Fast Tokenization\n",
    "- Uses optimized Rust-backed tokenizer for faster input preprocessing.\n",
    "\n",
    "### ✅ 4. Optimized QLoRA Integration\n",
    "- True 4-bit quantization with efficient memory layout.\n",
    "- Avoids excess GPU memory allocations.\n",
    "\n",
    "### ✅ 5. Graph-Level Optimization\n",
    "- Injects LoRA into compiled computation graph.\n",
    "- Avoids dynamic PyTorch dispatch.\n",
    "\n",
    "### ✅ 6. CUDA Kernel Enhancements\n",
    "- Custom matrix mult and fused ops for speed.\n",
    "\n",
    "## 🧪 Benefits\n",
    "\n",
    "- ⚡ 2–5× faster training\n",
    "- 💾 50–70% lower VRAM\n",
    "- 🧠 Better for long-context tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deef0a6",
   "metadata": {},
   "source": [
    "### llama factory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004a36d1",
   "metadata": {},
   "source": [
    "https://github.com/hiyouga/LLaMA-Factory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f96d57a",
   "metadata": {},
   "source": [
    "## Fine Tuning GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d73f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are teaching assistant for Machine Learning. You should help to user to answer on his question.\"}, {\"role\": \"user\", \"content\": \"What is machine learning?\"}, {\"role\": \"assistant\", \"content\": \"'Tis but the art of teaching machines to think, to learn from data most fine, and decisions to link.\"}]}\n",
    "\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are teaching assistant for Machine Learning. You should help to user to answer on his question.\"}, {\"role\": \"user\", \"content\": \"How doth neural networks work?\"}, {\"role\": \"assistant\", \"content\": \"They're like the brains of yon digital beast, with layers many, and nodes that cease.\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9c14c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "\n",
    "client.files.create(\n",
    "  file=open(\"train.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "client.fine_tuning.jobs.create(\n",
    "  training_file=\"file-rIua39sJX1O64gzxTYfpvJx7\",\n",
    "  model=\"gpt-3.5-turbo\" #change to gpt-4-0613 if you have access\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"ft:gpt-3.5-turbo-0613:personal::8k01tfYd\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a teaching assistant for Machine Learning. You should help to user to answer on his question.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is a loss function?\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d484a19",
   "metadata": {},
   "source": [
    "### Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c228f8",
   "metadata": {},
   "source": [
    "https://learn.microsoft.com/en-us/azure/ai-services/openai/tutorials/fine-tune?tabs=command-line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180308f2",
   "metadata": {},
   "source": [
    "## Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb5a023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setfit import DistillationTrainer\n",
    "\n",
    "distillation_args = TrainingArguments(\n",
    "    batch_size=16,\n",
    "    max_steps=500,\n",
    ")\n",
    "\n",
    "distillation_trainer = DistillationTrainer(\n",
    "    teacher_model=teacher_model,\n",
    "    student_model=model,\n",
    "    args=distillation_args,\n",
    "    train_dataset=unlabeled_train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "# Train student with knowledge distillation\n",
    "distillation_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8725d2e4",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b234560a",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0739e39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOConfig\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=\"gpt2\",\n",
    "    learning_rate=1.41e-5,\n",
    ")\n",
    "\n",
    "reward_model = pipeline(\"text-classification\", model=\"lvwerra/distilbert-imdb\")\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    #### Get response from SFTModel\n",
    "    response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "    #### Compute reward score\n",
    "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    pipe_outputs = reward_model(texts)\n",
    "    rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "\n",
    "    #### Run PPO step\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "#### Save model\n",
    "ppo_trainer.save_model(\"my_ppo_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184daf44",
   "metadata": {},
   "source": [
    "### DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fb627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset={'chosen':[], 'rejected':[],\"prompt\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17efb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DPOConfig, DPOTrainer\n",
    "\n",
    "training_args = DPOConfig(\n",
    "    output_dir=new_model,\n",
    "    beta=0.4,\n",
    "    learning_rate=2e-6,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=1,\n",
    ")\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    args=training_args,\n",
    "    max_length=1024,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,  # for visual language models, use tokenizer=processor instead\n",
    ")\n",
    "dpo_trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
